nTests trained baseline model on ALL conditions: clean + synthetic weather\nMeasures performance degradation to justify Phase 2 augmentation training\n\"\"\"\n\nimport os\nimport sys\nimport subprocess\nimport json\nimport yaml\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport logging","newString":"#!/usr/bin/env python3\n\"\"\"\nPhase 1 Baseline Weather Testing Script - YOLOv5n + VisDrone\nProtocol: extended_phase_protocol.md - Phase 1 Weather Degradation Analysis\n\nTests trained baseline model on ALL conditions: clean + synthetic weather\nMeasures performance degradation to justify Phase 2 augmentation training\n\"\"\"\n\nimport os\nimport sys\nimport subprocess\nimport json\nimport yaml\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport logging\nimport io\n\n# Fix Unicode encoding for Windows console\nif sys.platform.startswith('win'):\n    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')","originalFile":"#!/usr/bin/env python3\n\"\"\"\nPhase 1 Baseline Weather Testing Script - YOLOv5n + VisDrone\nProtocol: extended_phase_protocol.md - Phase 1 Weather Degradation Analysis\n\nTests trained baseline model on ALL conditions: clean + synthetic weather\nMeasures performance degradation to justify Phase 2 augmentation training\n\"\"\"\n\nimport os\nimport sys\nimport subprocess\nimport json\nimport yaml\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport logging\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('phase1_weather_testing.log'),\n        logging.StreamHandler(sys.stdout)\n    ]\n)\nlogger = logging.getLogger(__name__)\n\nclass Phase1WeatherTester:\n    \"\"\"Phase 1 Baseline Weather Conditions Tester\"\"\"\n    \n    def __init__(self):\n        self.project_root = Path(__file__).parent.parent.parent.parent.parent.parent\n        self.yolo_path = self.project_root / \"models\" / \"yolov5n\" / \"baseline\" / \"yolov5\"\n        self.weights_path = self.project_root / \"runs\" / \"phase1\" / \"yolov5n_baseline\" / \"weights\" / \"best.pt\"\n        self.results_dir = self.project_root / \"runs\" / \"phase1\" / \"yolov5n_baseline\" / \"weather_testing\"\n        \n        # Test conditions and their dataset paths\n        self.test_conditions = {\n            'clean': self.project_root / \"data\" / \"raw\" / \"visdrone\" / \"VisDrone2019-DET-test-dev\",\n            'fog': self.project_root / \"data\" / \"synthetic_test\" / \"VisDrone2019-DET-test-fog\",\n            'rain': self.project_root / \"data\" / \"synthetic_test\" / \"VisDrone2019-DET-test-rain\",\n            'night': self.project_root / \"data\" / \"synthetic_test\" / \"VisDrone2019-DET-test-night\",\n            'mixed': self.project_root / \"data\" / \"synthetic_test\" / \"VisDrone2019-DET-test-mixed\"\n        }\n        \n        # VisDrone class names\n        self.class_names = [\n            'pedestrian', 'people', 'bicycle', 'car', 'van', \n            'truck', 'tricycle', 'awning-tricycle', 'bus', 'motor'\n        ]\n        \n        # Base testing parameters\n        self.base_test_params = {\n            'weights': str(self.weights_path.absolute()),\n            'imgsz': 640,\n            'batch_size': 32,\n            'conf_thres': 0.001,\n            'iou_thres': 0.6,\n            'device': '0',\n            'workers': 8,\n            'augment': False,  # NO augmentation during testing\n            'verbose': True,\n            'save_txt': True,\n            'save_conf': True,\n            'save_json': True,\n            'exist_ok': True\n        }\n        \n        # Results storage\n        self.all_results = {}\n    \n    def validate_prerequisites(self):\n        \"\"\"Validate that training and synthetic test sets exist\"\"\"\n        logger.info(\"=== Phase 1 Weather Testing Prerequisites ===\")\n        \n        # Check YOLOv5 repository\n        if not self.yolo_path.exists():\n            raise FileNotFoundError(f\"YOLOv5 repository not found: {self.yolo_path}\")\n        logger.info(f\"✓ YOLOv5 repository: {self.yolo_path}\")\n        \n        # Check trained weights\n        if not self.weights_path.exists():\n            raise FileNotFoundError(f\"Trained weights not found: {self.weights_path}\")\n        logger.info(f\"✓ Trained weights: {self.weights_path}\")\n        \n        # Check all test datasets\n        for condition, path in self.test_conditions.items():\n            images_path = path / \"images\"\n            if not images_path.exists():\n                raise FileNotFoundError(f\"{condition} test images not found: {images_path}\")\n            \n            image_count = len(list(images_path.glob(\"*.jpg\")))\n            logger.info(f\"✓ {condition.capitalize()} test set: {image_count} images at {images_path}\")\n        \n        # Create results directory\n        self.results_dir.mkdir(parents=True, exist_ok=True)\n        logger.info(f\"✓ Results directory: {self.results_dir}\")\n        \n        logger.info(\"✓ Prerequisites validation passed\")\n    \n    def create_temp_dataset_config(self, condition, dataset_path):\n        \"\"\"Create temporary dataset config for specific test condition\"\"\"\n        temp_config = {\n            'path': str(dataset_path.absolute()),\n            'train': 'images',  # Not used for testing\n            'val': 'images',    # Not used for testing  \n            'test': 'images',   # This is what we test on\n            'names': {i: name for i, name in enumerate(self.class_names)},\n            'nc': len(self.class_names)\n        }\n        \n        temp_config_path = self.results_dir / f\"temp_{condition}_config.yaml\"\n        with open(temp_config_path, 'w') as f:\n            yaml.dump(temp_config, f, default_flow_style=False)\n        \n        return temp_config_path\n    \n    def test_single_condition(self, condition, dataset_path):\n        \"\"\"Test model on a single weather condition\"\"\"\n        logger.info(f\"=== Testing {condition.upper()} Condition ===\")\n        \n        # Create temporary config for this condition\n        temp_config = self.create_temp_dataset_config(condition, dataset_path)\n        \n        # Setup test parameters\n        test_params = self.base_test_params.copy()\n        test_params.update({\n            'data': str(temp_config.absolute()),\n            'project': str(self.results_dir.absolute()),\n            'name': f'test_{condition}'\n        })\n        \n        # Build command\n        cmd = [sys.executable, str(self.yolo_path / \"val.py\")]\n        \n        for key, value in test_params.items():\n            if isinstance(value, bool):\n                if value:\n                    cmd.append(f\"--{key.replace('_', '-')}\")\n            else:\n                cmd.extend([f\"--{key.replace('_', '-')}\", str(value)])\n        \n        logger.info(f\"Testing command: {' '.join(cmd)}\")\n        \n        # Change to YOLOv5 directory and execute\n        original_cwd = os.getcwd()\n        os.chdir(self.yolo_path)\n        \n        try:\n            # Run testing\n            process = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                universal_newlines=True,\n                bufsize=1\n            )\n            \n            # Stream output\n            output_lines = []\n            for line in process.stdout:\n                line = line.rstrip()\n                print(line)\n                output_lines.append(line)\n                logger.info(line)\n            \n            # Wait for completion\n            return_code = process.wait()\n            \n            if return_code == 0:\n                logger.info(f\"✓ {condition} testing completed successfully\")\n                return self.extract_metrics_from_output(output_lines, condition)\n            else:\n                raise RuntimeError(f\"{condition} testing failed with return code {return_code}\")\n                \n        except Exception as e:\n            logger.error(f\"{condition} testing failed: {str(e)}\")\n            raise\n        finally:\n            os.chdir(original_cwd)\n            # Clean up temporary config\n            if temp_config.exists():\n                temp_config.unlink()\n    \n    def extract_metrics_from_output(self, output_lines, condition):\n        \"\"\"Extract metrics from YOLOv5 output\"\"\"\n        metrics = {\n            'condition': condition,\n            'mAP_50': 0.0,\n            'mAP_50_95': 0.0,\n            'precision': 0.0,\n            'recall': 0.0,\n            'f1': 0.0\n        }\n        \n        # Look for results in output\n        for line in output_lines:\n            if 'all' in line and 'mAP' in line:\n                # Parse typical YOLOv5 results line\n                # Format: \"all  1610  ...  0.123  0.456  0.789  0.101\"\n                try:\n                    parts = line.split()\n                    if len(parts) >= 6:\n                        metrics['precision'] = float(parts[-4])\n                        metrics['recall'] = float(parts[-3])\n                        metrics['mAP_50'] = float(parts[-2])\n                        metrics['mAP_50_95'] = float(parts[-1])\n                        metrics['f1'] = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall']) if (metrics['precision'] + metrics['recall']) > 0 else 0\n                        break\n                except (ValueError, IndexError):\n                    continue\n        \n        # Try to read from CSV if available\n        results_csv = self.results_dir / f\"test_{condition}\" / \"results.csv\"\n        if results_csv.exists():\n            try:\n                df = pd.read_csv(results_csv)\n                if not df.empty:\n                    last_row = df.iloc[-1]\n                    metrics['mAP_50'] = last_row.get('metrics/mAP_0.5', metrics['mAP_50'])\n                    metrics['mAP_50_95'] = last_row.get('metrics/mAP_0.5:0.95', metrics['mAP_50_95'])\n                    metrics['precision'] = last_row.get('metrics/precision', metrics['precision'])\n                    metrics['recall'] = last_row.get('metrics/recall', metrics['recall'])\n            except Exception as e:\n                logger.warning(f\"Could not read CSV for {condition}: {e}\")\n        \n        logger.info(f\"{condition} results - mAP@0.5: {metrics['mAP_50']:.4f}, mAP@0.5:0.95: {metrics['mAP_50_95']:.4f}\")\n        return metrics\n    \n    def calculate_degradation(self, clean_metrics, weather_metrics):\n        \"\"\"Calculate performance degradation percentage\"\"\"\n        degradation = {}\n        \n        for metric in ['mAP_50', 'mAP_50_95', 'precision', 'recall', 'f1']:\n            clean_value = clean_metrics.get(metric, 0)\n            weather_value = weather_metrics.get(metric, 0)\n            \n            if clean_value > 0:\n                degradation_pct = ((clean_value - weather_value) / clean_value) * 100\n                degradation[f'{metric}_degradation'] = degradation_pct\n            else:\n                degradation[f'{metric}_degradation'] = 0\n        \n        return degradation\n    \n    def run_all_tests(self):\n        \"\"\"Run testing on all weather conditions\"\"\"\n        logger.info(\"=== Running All Weather Condition Tests ===\")\n        \n        # Test each condition\n        for condition, dataset_path in self.test_conditions.items():\n            try:\n                metrics = self.test_single_condition(condition, dataset_path)\n                self.all_results[condition] = metrics\n                \n                logger.info(f\"✓ {condition} testing complete\")\n                \n            except Exception as e:\n                logger.error(f\"Failed to test {condition}: {e}\")\n                # Continue with other conditions\n                self.all_results[condition] = {\n                    'condition': condition,\n                    'mAP_50': 0.0,\n                    'mAP_50_95': 0.0,\n                    'precision': 0.0,\n                    'recall': 0.0,\n                    'f1': 0.0,\n                    'error': str(e)\n                }\n        \n        # Calculate degradations\n        if 'clean' in self.all_results:\n            clean_metrics = self.all_results['clean']\n            \n            for condition in ['fog', 'rain', 'night', 'mixed']:\n                if condition in self.all_results:\n                    weather_metrics = self.all_results[condition]\n                    degradation = self.calculate_degradation(clean_metrics, weather_metrics)\n                    self.all_results[condition].update(degradation)\n    \n    def generate_comparison_report(self):\n        \"\"\"Generate comprehensive comparison report\"\"\"\n        logger.info(\"=== Generating Weather Degradation Report ===\")\n        \n        # Create DataFrame for analysis\n        df_data = []\n        for condition, metrics in self.all_results.items():\n            df_data.append(metrics)\n        \n        df = pd.DataFrame(df_data)\n        \n        # Save detailed results\n        results_csv = self.results_dir / \"weather_degradation_analysis.csv\"\n        df.to_csv(results_csv, index=False)\n        logger.info(f\"✓ Detailed results saved: {results_csv}\")\n        \n        # Create summary table\n        summary_data = []\n        if 'clean' in self.all_results:\n            clean_metrics = self.all_results['clean']\n            \n            for condition in ['clean', 'fog', 'rain', 'night', 'mixed']:\n                if condition in self.all_results:\n                    metrics = self.all_results[condition]\n                    row = {\n                        'Condition': condition.capitalize(),\n                        'mAP@0.5': f\"{metrics['mAP_50']:.4f}\",\n                        'mAP@0.5:0.95': f\"{metrics['mAP_50_95']:.4f}\",\n                        'Precision': f\"{metrics['precision']:.4f}\",\n                        'Recall': f\"{metrics['recall']:.4f}\"\n                    }\n                    \n                    if condition != 'clean' and f'mAP_50_degradation' in metrics:\n                        row['mAP@0.5 Degradation'] = f\"{metrics['mAP_50_degradation']:.1f}%\"\n                    else:\n                        row['mAP@0.5 Degradation'] = \"Baseline\"\n                    \n                    summary_data.append(row)\n        \n        summary_df = pd.DataFrame(summary_data)\n        \n        # Generate report\n        report = f\"\"\"\n# Phase 1 Baseline Weather Degradation Analysis\n## YOLOv5n Performance Across Weather Conditions\n\n**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n**Model**: YOLOv5n (No Augmentation - Baseline)\n**Dataset**: VisDrone Test Sets (Clean + Synthetic Weather)\n**Test Images**: 1,610 per condition\n\n## Performance Summary\n\n{summary_df.to_markdown(index=False)}\n\n## Key Findings\n\n### Performance Degradation Analysis\n\"\"\"\n        \n        if 'clean' in self.all_results:\n            clean_map = self.all_results['clean']['mAP_50']\n            \n            for condition in ['fog', 'rain', 'night', 'mixed']:\n                if condition in self.all_results and f'mAP_50_degradation' in self.all_results[condition]:\n                    degradation = self.all_results[condition]['mAP_50_degradation']\n                    weather_map = self.all_results[condition]['mAP_50']\n                    report += f\"- **{condition.capitalize()}**: {weather_map:.4f} mAP@0.5 ({degradation:.1f}% degradation)\\\\n\"\n        \n        report += f\"\"\"\n\n### Statistical Analysis\n- **Baseline Performance**: {self.all_results.get('clean', {}).get('mAP_50', 0):.4f} mAP@0.5\n- **Worst Degradation**: Night conditions (expected due to reduced visibility)\n- **Best Weather Robustness**: Analysis shows need for augmentation training\n\n### Thesis Implications\n1. **Baseline Established**: Clean performance provides reference point\n2. **Weather Vulnerability**: Significant degradation across all weather conditions\n3. **Phase 2 Justification**: Results demonstrate need for weather augmentation training\n4. **Research Contribution**: Quantified impact of weather on drone object detection\n\n## Methodology\n- Model trained on clean VisDrone training set (NO augmentation)\n- Tested on clean validation set to establish baseline\n- Tested on synthetic weather conditions (fog, rain, night, mixed)\n- All tests used identical parameters (conf=0.001, IoU=0.6)\n- No augmentation applied during testing (fair comparison)\n\n## Next Steps\n1. **Phase 2 Training**: Implement weather augmentation during training\n2. **Comparative Analysis**: Measure improvement with augmentation\n3. **Statistical Validation**: Multiple runs for confidence intervals\n4. **Publication**: Results ready for thesis and potential publication\n\n---\n*Generated by Phase 1 Baseline Testing Pipeline*\n*Model weights: `runs/phase1/yolov5n_baseline/weights/best.pt`*\n\"\"\"\n        \n        # Save report\n        report_file = self.results_dir / \"weather_degradation_report.md\"\n        with open(report_file, 'w') as f:\n            f.write(report)\n        \n        logger.info(f\"✓ Weather degradation report saved: {report_file}\")\n        \n        # Save results as JSON\n        json_file = self.results_dir / \"all_weather_results.json\"\n        with open(json_file, 'w') as f:\n            json.dump(self.all_results, f, indent=2)\n        logger.info(f\"✓ JSON results saved: {json_file}\")\n        \n        return summary_df\n    \n    def create_visualization(self):\n        \"\"\"Create performance visualization charts\"\"\"\n        logger.info(\"=== Creating Performance Visualizations ===\")\n        \n        if not self.all_results:\n            logger.warning(\"No results available for visualization\")\n            return\n        \n        # Setup plot style\n        plt.style.use('default')\n        sns.set_palette(\"husl\")\n        \n        # Create figure with subplots\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n        fig.suptitle('Phase 1 Baseline: Weather Impact on YOLOv5n Performance', fontsize=16, fontweight='bold')\n        \n        # Prepare data\n        conditions = []\n        map_50 = []\n        map_50_95 = []\n        precision = []\n        recall = []\n        \n        for condition in ['clean', 'fog', 'rain', 'night', 'mixed']:\n            if condition in self.all_results:\n                conditions.append(condition.capitalize())\n                map_50.append(self.all_results[condition]['mAP_50'])\n                map_50_95.append(self.all_results[condition]['mAP_50_95'])\n                precision.append(self.all_results[condition]['precision'])\n                recall.append(self.all_results[condition]['recall'])\n        \n        # Plot 1: mAP@0.5 comparison\n        bars1 = ax1.bar(conditions, map_50, color=['green' if c == 'Clean' else 'red' for c in conditions])\n        ax1.set_title('mAP@0.5 Across Weather Conditions', fontweight='bold')\n        ax1.set_ylabel('mAP@0.5')\n        ax1.set_ylim(0, max(map_50) * 1.1 if map_50 else 1)\n        \n        # Add value labels on bars\n        for bar, value in zip(bars1, map_50):\n            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n        \n        # Plot 2: mAP@0.5:0.95 comparison\n        bars2 = ax2.bar(conditions, map_50_95, color=['green' if c == 'Clean' else 'orange' for c in conditions])\n        ax2.set_title('mAP@0.5:0.95 Across Weather Conditions', fontweight='bold')\n        ax2.set_ylabel('mAP@0.5:0.95')\n        ax2.set_ylim(0, max(map_50_95) * 1.1 if map_50_95 else 1)\n        \n        for bar, value in zip(bars2, map_50_95):\n            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n        \n        # Plot 3: Precision vs Recall\n        colors = ['green', 'red', 'orange', 'purple', 'brown']\n        for i, (cond, prec, rec, color) in enumerate(zip(conditions, precision, recall, colors)):\n            ax3.scatter(rec, prec, s=200, c=color, label=cond, alpha=0.7)\n            ax3.annotate(cond, (rec, prec), xytext=(5, 5), textcoords='offset points')\n        \n        ax3.set_title('Precision vs Recall', fontweight='bold')\n        ax3.set_xlabel('Recall')\n        ax3.set_ylabel('Precision')\n        ax3.legend()\n        ax3.grid(True, alpha=0.3)\n        \n        # Plot 4: Degradation percentage\n        if 'clean' in self.all_results:\n            degradation_conditions = []\n            degradation_values = []\n            \n            for condition in ['fog', 'rain', 'night', 'mixed']:\n                if condition in self.all_results and f'mAP_50_degradation' in self.all_results[condition]:\n                    degradation_conditions.append(condition.capitalize())\n                    degradation_values.append(self.all_results[condition]['mAP_50_degradation'])\n            \n            if degradation_values:\n                bars4 = ax4.bar(degradation_conditions, degradation_values, color='red', alpha=0.7)\n                ax4.set_title('Performance Degradation (mAP@0.5)', fontweight='bold')\n                ax4.set_ylabel('Degradation (%)')\n                ax4.set_ylim(0, max(degradation_values) * 1.1 if degradation_values else 100)\n                \n                for bar, value in zi